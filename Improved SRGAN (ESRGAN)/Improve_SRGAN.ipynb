{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "p1BvR2hZDi87"
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "\r\n",
    "DIV2K_RGB_MEAN = np.array([0.4488, 0.4371, 0.4040]) * 255\r\n",
    "\r\n",
    "\r\n",
    "def resolve_single(model, lr):\r\n",
    "    return resolve(model, tf.expand_dims(lr, axis=0))[0]\r\n",
    "\r\n",
    "\r\n",
    "def resolve(model, lr_batch):\r\n",
    "    lr_batch = tf.cast(lr_batch, tf.float32)\r\n",
    "    sr_batch = model(lr_batch)\r\n",
    "    sr_batch = tf.clip_by_value(sr_batch, 0, 255)\r\n",
    "    sr_batch = tf.round(sr_batch)\r\n",
    "    sr_batch = tf.cast(sr_batch, tf.uint8)\r\n",
    "    return sr_batch\r\n",
    "\r\n",
    "def normalize(x, rgb_mean=DIV2K_RGB_MEAN):\r\n",
    "    return (x - rgb_mean) / 127.5\r\n",
    "\r\n",
    "\r\n",
    "def denormalize(x, rgb_mean=DIV2K_RGB_MEAN):\r\n",
    "    return x * 127.5 + rgb_mean\r\n",
    "\r\n",
    "\r\n",
    "def normalize_01(x):\r\n",
    "    return x / 255.0\r\n",
    "\r\n",
    "\r\n",
    "def normalize_m11(x):\r\n",
    "    return x / 127.5 - 1\r\n",
    "\r\n",
    "\r\n",
    "def denormalize_m11(x):\r\n",
    "    return (x + 1) * 127.5\r\n",
    "\r\n",
    "def pixel_shuffle(scale):\r\n",
    "    return lambda x: tf.nn.depth_to_space(x, scale)\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "CUH3CoWtDvbo"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Add, BatchNormalization, Conv2D, Dense, Flatten, Input, LeakyReLU, PReLU, Lambda, Activation, Concatenate, Multiply, Dropout\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.applications.vgg19 import VGG19\n",
    "\n",
    "LR_SIZE = 24\n",
    "HR_SIZE = 96\n",
    "upscaling_factor = 4\n",
    "channels = 3\n",
    "filters = 64\n",
    "\n",
    "def SubpixelConv2D(name, scale=2):\n",
    "        \n",
    "        def subpixel_shape(input_shape):\n",
    "            dims = [input_shape[0],\n",
    "                    None if input_shape[1] is None else input_shape[1] * scale,\n",
    "                    None if input_shape[2] is None else input_shape[2] * scale,\n",
    "                    int(input_shape[3] / (scale ** 2))]\n",
    "            output_shape = tuple(dims)\n",
    "            return output_shape\n",
    "\n",
    "        def subpixel(x):\n",
    "            return tf.nn.depth_to_space(x, scale)\n",
    "\n",
    "        return Lambda(subpixel, output_shape=subpixel_shape, name=name)\n",
    "\n",
    "def upsample(x, number):\n",
    "    x = Conv2D(256, kernel_size=3, strides=1, padding='same', name='upSampleConv2d_' + str(number))(x)\n",
    "    x = SubpixelConv2D(name = str('upSampleSubPixel_') + str(number) , scale = 2)(x)\n",
    "    x = PReLU(shared_axes=[1, 2], name='upSamplePReLU_' + str(number))(x)\n",
    "    return x\n",
    "\n",
    "def dense_block(input):\n",
    "            \n",
    "  x1 = Conv2D(64, kernel_size=3, strides=1, padding='same')(input)\n",
    "  x1 = LeakyReLU(0.2)(x1)\n",
    "  x1 = Concatenate()([input, x1])\n",
    "\n",
    "  x2 = Conv2D(64, kernel_size=3, strides=1, padding='same')(x1)\n",
    "  x2 = LeakyReLU(0.2)(x2)\n",
    "  x2 = Concatenate()([input, x1, x2])\n",
    "\n",
    "  x3 = Conv2D(64, kernel_size=3, strides=1, padding='same')(x2)\n",
    "  x3 = LeakyReLU(0.2)(x3)\n",
    "  x3 = Concatenate()([input, x1, x2, x3])\n",
    "\n",
    "  x4 = Conv2D(64, kernel_size=3, strides=1, padding='same')(x3)\n",
    "  x4 = LeakyReLU(0.2)(x4)\n",
    "  x4 = Concatenate()([input, x1, x2, x3, x4])  \n",
    "\n",
    "  x5 = Conv2D(64, kernel_size=3, strides=1, padding='same')(x4)\n",
    "  x5 = Lambda(lambda x: x * 0.2)(x5)\n",
    "  x = Add()([x5, input])\n",
    "  return x\n",
    "\n",
    "def RRDB(input):\n",
    "    x = dense_block(input)\n",
    "    x = dense_block(x)\n",
    "    x = dense_block(x)\n",
    "    x = Lambda(lambda x: x * 0.2)(x)\n",
    "    out = Add()([x, input])\n",
    "    return out\n",
    "\n",
    "\n",
    "def sr_resnet(num_filters=64, num_res_blocks=16):\n",
    "    lr_input = Input(shape=(24, 24, 3))\n",
    "\n",
    "    x_start = Conv2D(64, kernel_size=3, strides=1, padding='same')(lr_input)\n",
    "    x_start = LeakyReLU(0.2)(x_start)\n",
    "\n",
    "    x = RRDB(x_start)\n",
    "\n",
    "    x = Conv2D(64, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = Lambda(lambda x: x * 0.2)(x)\n",
    "    x = Add()([x, x_start])\n",
    "\n",
    "    x = upsample(x, 1)\n",
    "    if upscaling_factor > 2:\n",
    "            x = upsample(x, 2)\n",
    "    if upscaling_factor > 4:\n",
    "            x = upsample(x, 3)\n",
    "\n",
    "    x = Conv2D(64, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    hr_output = Conv2D(channels, kernel_size=3, strides=1, padding='same', activation='tanh')(x)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=lr_input, outputs=hr_output)\n",
    "  \n",
    "    return model\n",
    "\n",
    "\n",
    "generator = sr_resnet\n",
    "\n",
    "\n",
    "def conv2d_block(input, filters, strides=1, bn=True):\n",
    "      \n",
    "      d = Conv2D(filters, kernel_size=3, strides=strides, padding='same')(input)\n",
    "      d = LeakyReLU(alpha=0.2)(d)\n",
    "      if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "      return d\n",
    "\n",
    "\n",
    "\n",
    "def discriminator(num_filters=64):\n",
    "        img = Input(shape=(HR_SIZE,HR_SIZE,3))\n",
    "        x = conv2d_block(img, filters, bn=False)\n",
    "        x = conv2d_block(x, filters, strides=2)\n",
    "        x = conv2d_block(x, filters * 2)\n",
    "        x = conv2d_block(x, filters * 2, strides=2)\n",
    "        x = conv2d_block(x, filters * 4)\n",
    "        x = conv2d_block(x, filters * 4, strides=2)\n",
    "        x = conv2d_block(x, filters * 8)\n",
    "        x = conv2d_block(x, filters * 8, strides=2)\n",
    "        x = Dense(filters * 16)(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = Dropout(0.4)(x)\n",
    "        x = Dense(1)(x)\n",
    "\n",
    "        # Create model and compile\n",
    "        model = Model(inputs=img, outputs=x)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QF59Hg3Gmhjr",
    "outputId": "e52d0c02-68d3-434e-a756-ea2af57bbb49"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "\n",
    "# This is the TPU initialization code that has to be at the beginning.\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n",
    "\r\n",
    "with strategy.scope():\r\n",
    "  generator().summary()\r\n",
    "  discriminator().summary()\r\n",
    "  model = tf.keras.models.Sequential()\r\n",
    "  model.add(generator())\r\n",
    "  model.add(discriminator())\r\n",
    "  model.summary()\r\n",
    "  discriminator().compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\r\n",
    "  discriminator().trainable = False\r\n",
    "  model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LG6hkJuPD7c5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\r\n",
    "import cv2\r\n",
    "\r\n",
    "batch_size = 32\r\n",
    "\r\n",
    "def train_dcgan(model,epochs=5):\r\n",
    "   \r\n",
    "\r\n",
    "    print(\"done\")\r\n",
    "    generator, discriminator = model.layers\r\n",
    "    discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\r\n",
    "    discriminator.trainable = False\r\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\r\n",
    "    path = '/content/drive/MyDrive/train/'\r\n",
    "    for epoch in tqdm(range(epochs)):\r\n",
    "      print(\"Epoch {}/{}\".format(epoch + 1, epochs))\r\n",
    "      for root, dirnames, filenames in os.walk(path):\r\n",
    "        i = 0\r\n",
    "        j = 0\r\n",
    "        x_train_x = np.zeros((32,24,24,3))\r\n",
    "        x_train_y = np.zeros((32,96,96,3))\r\n",
    "        for filename in filenames:\r\n",
    "          img_path = os.path.join(path,filename)\r\n",
    "          x_train = cv2.imread(img_path)\r\n",
    "          x_trainx = cv2.resize(x_train,(24,24))\r\n",
    "          x_trainy = cv2.resize(x_train,(96,96))\r\n",
    "          x_train_x[i] = x_trainx \r\n",
    "          x_train_y[i] = x_trainy\r\n",
    "          i = i+1\r\n",
    "          if i == 32:\r\n",
    "            j = j + 1\r\n",
    "            print(\"batch {}/254\".format(j))\r\n",
    "            X_batch, Y_batch = x_train_x, x_train_y\r\n",
    "            X_batch = tf.cast(X_batch, tf.float32)\r\n",
    "            Y_batch = tf.cast(Y_batch, tf.float32)\r\n",
    "            generated_images = generator(X_batch)\r\n",
    "            X = tf.cast(generated_images, tf.float32) \r\n",
    "            X_fake_and_real = tf.concat([X, Y_batch], axis=0)\r\n",
    "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\r\n",
    "            discriminator.trainable = True\r\n",
    "            history_disc = discriminator.train_on_batch(X_fake_and_real, y1)\r\n",
    "            y2 = tf.constant([[1.]] * batch_size)\r\n",
    "            discriminator.trainable = False\r\n",
    "            history_gen = model.train_on_batch(X_batch, y2)\r\n",
    "            i = 0\r\n",
    "            x_train_x = np.zeros((32,24,24,3))\r\n",
    "            x_train_y = np.zeros((32,96,96,3))\r\n",
    "    \r\n",
    "      if (epoch+1)%10 == 0:\r\n",
    "\r\n",
    "        generator.save_weights(\"Generator{}.h5\".format(epoch))\r\n",
    "        discriminator.save_weights(\"Discriminator_weights{}.h5\".format(epoch))\r\n",
    "        model.save_weights(\"Model{}.h5\".format(epoch))\r\n",
    "        from google.colab.patches import cv2_imshow\r\n",
    "\r\n",
    "        path = \"/content/drive/MyDrive/train/07336.jpg\"\r\n",
    "\r\n",
    "        X = cv2.imread(path)\r\n",
    "        X = cv2.resize(X,(24,24))\r\n",
    "        X = np.reshape(X, (1,24,24,3))\r\n",
    "        X_batch = tf.cast(X, tf.float32)\r\n",
    "\r\n",
    "        generator.load_weights(\"Generator{}.h5\".format(epoch))\r\n",
    "        discriminator.load_weights(\"Discriminator_weights{}.h5\".format(epoch))\r\n",
    "        Y = generator(X_batch)\r\n",
    "        cv2_imshow(X[0])\r\n",
    "        cv2_imshow( Y[0].numpy())\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dcgan(model,epochs=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 960
    },
    "id": "oUwsshjjyFua",
    "outputId": "621b0314-12b3-446b-f043-5ea5b92a2ac2"
   },
   "outputs": [],
   "source": [
    "import cv2\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from google.colab.patches import cv2_imshow\r\n",
    "\r\n",
    "path = \"/content/drive/MyDrive/train/04336.jpg\"\r\n",
    "\r\n",
    "X2 = cv2.imread(path)\r\n",
    "X1 = cv2.resize(X2,(24,24), interpolation = cv2.INTER_AREA)\r\n",
    "X = np.reshape(X1, (1,24,24,3))\r\n",
    "X_batch = tf.cast(X, tf.float32)\r\n",
    "\r\n",
    "generator, discriminator = model.layers\r\n",
    "#generator.load_weights(\"DIV2K_gan.h5\")\r\n",
    "generator.load_weights(\"/content/generator_4X_epoch65000.h5\")\r\n",
    "Y = generator(X_batch/255)\r\n",
    "cv2_imshow(X[0])\r\n",
    "output = (Y[0].numpy() + 1)*127.5\r\n",
    "cv2_imshow( output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%inline\n",
    "loss_disc = history_disc.history['loss']\n",
    "loss_gen = history_gen.history['loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 1, 1)\n",
    "\n",
    "plt.plot(epochs_range, loss_gen, label='Generator Loss')\n",
    "plt.plot(epochs_range, loss_disc, label='Discriminator Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Improve_SRGan",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}