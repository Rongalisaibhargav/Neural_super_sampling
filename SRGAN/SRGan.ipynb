{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "p1BvR2hZDi87"
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "\r\n",
    "DIV2K_RGB_MEAN = np.array([0.4488, 0.4371, 0.4040]) * 255\r\n",
    "\r\n",
    "\r\n",
    "def resolve_single(model, lr):\r\n",
    "    return resolve(model, tf.expand_dims(lr, axis=0))[0]\r\n",
    "\r\n",
    "\r\n",
    "def resolve(model, lr_batch):\r\n",
    "    lr_batch = tf.cast(lr_batch, tf.float32)\r\n",
    "    sr_batch = model(lr_batch)\r\n",
    "    sr_batch = tf.clip_by_value(sr_batch, 0, 255)\r\n",
    "    sr_batch = tf.round(sr_batch)\r\n",
    "    sr_batch = tf.cast(sr_batch, tf.uint8)\r\n",
    "    return sr_batch\r\n",
    "\r\n",
    "def normalize(x, rgb_mean=DIV2K_RGB_MEAN):\r\n",
    "    return (x - rgb_mean) / 127.5\r\n",
    "\r\n",
    "\r\n",
    "def denormalize(x, rgb_mean=DIV2K_RGB_MEAN):\r\n",
    "    return x * 127.5 + rgb_mean\r\n",
    "\r\n",
    "\r\n",
    "def normalize_01(x):\r\n",
    "    return x / 255.0\r\n",
    "\r\n",
    "\r\n",
    "def normalize_m11(x):\r\n",
    "    return x / 127.5 - 1\r\n",
    "\r\n",
    "\r\n",
    "def denormalize_m11(x):\r\n",
    "    return (x + 1) * 127.5\r\n",
    "\r\n",
    "def pixel_shuffle(scale):\r\n",
    "    return lambda x: tf.nn.depth_to_space(x, scale)\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CUH3CoWtDvbo"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Add, BatchNormalization, Conv2D, Dense, Flatten, Input, LeakyReLU, PReLU, Lambda\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.applications.vgg19 import VGG19\n",
    "\n",
    "LR_SIZE = 24\n",
    "HR_SIZE = 96\n",
    "\n",
    "\n",
    "def upsample(x_in, num_filters):\n",
    "    x = Conv2D(num_filters, kernel_size=3, padding='same')(x_in)\n",
    "    x = Lambda(pixel_shuffle(scale=2))(x)\n",
    "    return PReLU(shared_axes=[1, 2])(x)\n",
    "\n",
    "\n",
    "def res_block(x_in, num_filters, momentum=0.8):\n",
    "    x = Conv2D(num_filters, kernel_size=3, padding='same')(x_in)\n",
    "    x = BatchNormalization(momentum=momentum)(x)\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "    x = Conv2D(num_filters, kernel_size=3, padding='same')(x)\n",
    "    x = BatchNormalization(momentum=momentum)(x)\n",
    "    x = Add()([x_in, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def sr_resnet(num_filters=64, num_res_blocks=16):\n",
    "    x_in = Input(shape=(LR_SIZE, LR_SIZE, 3))\n",
    "    x = Lambda(normalize_01)(x_in)\n",
    "\n",
    "    x = Conv2D(num_filters, kernel_size=9, padding='same')(x)\n",
    "    x = x_1 = PReLU(shared_axes=[1, 2])(x)\n",
    "\n",
    "    for _ in range(num_res_blocks):\n",
    "        x = res_block(x, num_filters)\n",
    "\n",
    "    x = Conv2D(num_filters, kernel_size=3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x_1, x])\n",
    "\n",
    "    x = upsample(x, num_filters * 4)\n",
    "    x = upsample(x, num_filters * 4)\n",
    "\n",
    "    x = Conv2D(3, kernel_size=9, padding='same', activation='tanh')(x)\n",
    "    x = Lambda(denormalize_m11)(x)\n",
    "\n",
    "    return Model(x_in, x)\n",
    "\n",
    "\n",
    "generator = sr_resnet\n",
    "\n",
    "\n",
    "def discriminator_block(x_in, num_filters, strides=1, batchnorm=True, momentum=0.8):\n",
    "    x = Conv2D(num_filters, kernel_size=3, strides=strides, padding='same')(x_in)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization(momentum=momentum)(x)\n",
    "    return LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "\n",
    "def discriminator(num_filters=64):\n",
    "    x_in = Input(shape=(HR_SIZE, HR_SIZE, 3))\n",
    "    x = Lambda(normalize_m11)(x_in)\n",
    "\n",
    "    x = discriminator_block(x, num_filters, batchnorm=False)\n",
    "    x = discriminator_block(x, num_filters, strides=2)\n",
    "\n",
    "    x = discriminator_block(x, num_filters * 2)\n",
    "    x = discriminator_block(x, num_filters * 2, strides=2)\n",
    "\n",
    "    x = discriminator_block(x, num_filters * 4)\n",
    "    x = discriminator_block(x, num_filters * 4, strides=2)\n",
    "\n",
    "    x = discriminator_block(x, num_filters * 8)\n",
    "    x = discriminator_block(x, num_filters * 8, strides=2)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(1024)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(x_in, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QF59Hg3Gmhjr",
    "outputId": "8c409eea-4c99-4af5-ba8d-7e4f3c536d14"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "\n",
    "# This is the TPU initialization code that has to be at the beginning.\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n",
    "\r\n",
    "with strategy.scope():\r\n",
    "  generator().summary()\r\n",
    "  discriminator().summary()\r\n",
    "  model = tf.keras.models.Sequential()\r\n",
    "  model.add(generator())\r\n",
    "  model.add(discriminator())\r\n",
    "  model.summary()\r\n",
    "  discriminator().compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\r\n",
    "  discriminator().trainable = False\r\n",
    "  model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LG6hkJuPD7c5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\r\n",
    "import cv2\r\n",
    "\r\n",
    "batch_size = 32\r\n",
    "\r\n",
    "def train_dcgan(model,epochs=5):\r\n",
    "   \r\n",
    "\r\n",
    "    print(\"done\")\r\n",
    "    generator, discriminator = model.layers\r\n",
    "    discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\r\n",
    "    discriminator.trainable = False\r\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\r\n",
    "    path = '/content/drive/MyDrive/train/'\r\n",
    "    for epoch in tqdm(range(epochs)):\r\n",
    "      print(\"Epoch {}/{}\".format(epoch + 1, epochs))\r\n",
    "      for root, dirnames, filenames in os.walk(path):\r\n",
    "        i = 0\r\n",
    "        j = 0\r\n",
    "        x_train_x = np.zeros((32,24,24,3))\r\n",
    "        x_train_y = np.zeros((32,96,96,3))\r\n",
    "        for filename in filenames:\r\n",
    "          img_path = os.path.join(path,filename)\r\n",
    "          x_train = cv2.imread(img_path)\r\n",
    "          x_trainx = cv2.resize(x_train,(24,24))\r\n",
    "          x_trainy = cv2.resize(x_train,(96,96))\r\n",
    "          x_train_x[i] = x_trainx \r\n",
    "          x_train_y[i] = x_trainy\r\n",
    "          i = i+1\r\n",
    "          if i == 32:\r\n",
    "            j = j + 1\r\n",
    "            print(\"batch {}/254\".format(j))\r\n",
    "            X_batch, Y_batch = x_train_x, x_train_y\r\n",
    "            X_batch = tf.cast(X_batch, tf.float32)\r\n",
    "            Y_batch = tf.cast(Y_batch, tf.float32)\r\n",
    "            generated_images = generator(X_batch)\r\n",
    "            X = tf.cast(generated_images, tf.float32) \r\n",
    "            X_fake_and_real = tf.concat([X, Y_batch], axis=0)\r\n",
    "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\r\n",
    "            discriminator.trainable = True\r\n",
    "            history_disc = discriminator.train_on_batch(X_fake_and_real, y1)\r\n",
    "            y2 = tf.constant([[1.]] * batch_size)\r\n",
    "            discriminator.trainable = False\r\n",
    "            history_gen = model.train_on_batch(X_batch, y2)\r\n",
    "            i = 0\r\n",
    "            x_train_x = np.zeros((32,24,24,3))\r\n",
    "            x_train_y = np.zeros((32,96,96,3))\r\n",
    "    \r\n",
    "      if (epoch+1)%10 == 0:\r\n",
    "\r\n",
    "        generator.save_weights(\"Generator{}.h5\".format(epoch))\r\n",
    "        discriminator.save_weights(\"Discriminator_weights{}.h5\".format(epoch))\r\n",
    "        model.save_weights(\"Model{}.h5\".format(epoch))\r\n",
    "        from google.colab.patches import cv2_imshow\r\n",
    "\r\n",
    "        path = \"/content/drive/MyDrive/train/07336.jpg\"\r\n",
    "\r\n",
    "        X = cv2.imread(path)\r\n",
    "        X = cv2.resize(X,(24,24))\r\n",
    "        X = np.reshape(X, (1,24,24,3))\r\n",
    "        X_batch = tf.cast(X, tf.float32)\r\n",
    "\r\n",
    "        generator.load_weights(\"Generator{}.h5\".format(epoch))\r\n",
    "        discriminator.load_weights(\"Discriminator_weights{}.h5\".format(epoch))\r\n",
    "        Y = generator(X_batch)\r\n",
    "        cv2_imshow(X[0])\r\n",
    "        cv2_imshow( Y[0].numpy())\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2200\r\n",
    "train_dcgan(model,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "id": "oUwsshjjyFua",
    "outputId": "534d4b80-959a-49b6-b2c0-2bc3370e4471"
   },
   "outputs": [],
   "source": [
    "import cv2\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from google.colab.patches import cv2_imshow\r\n",
    "\r\n",
    "path = \"/content/drive/MyDrive/train/02284.jpg\"\r\n",
    "\r\n",
    "X2 = cv2.imread(path)\r\n",
    "X1 = cv2.resize(X2,(24,24), interpolation = cv2.INTER_AREA)\r\n",
    "X = np.reshape(X1, (1,24,24,3))\r\n",
    "X_batch = tf.cast(X, tf.float32)\r\n",
    "\r\n",
    "generator, discriminator = model.layers\r\n",
    "generator.load_weights(\"gan_generator.h5\")\r\n",
    "Y = generator(X_batch)\r\n",
    "cv2_imshow(X[0])\r\n",
    "cv2_imshow( Y[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%inline\n",
    "loss_disc = history_disc.history['loss']\n",
    "loss_gen = history_gen.history['loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 1, 1)\n",
    "\n",
    "plt.plot(epochs_range, loss_gen, label='Generator Loss')\n",
    "plt.plot(epochs_range, loss_disc, label='Discriminator Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "SRGan.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "1e291ab98d7d63baf1917f7f8178a754e3ec8935fa55d644c46b0916a12b31d8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}